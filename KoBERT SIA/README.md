# KoBERT-nsmc

- KoBERTë¥¼ ì´ìš©í•œ ë„¤ì´ë²„ ì˜í™” ë¦¬ë·° ê°ì • ë¶„ì„ (sentiment classification)
- ğŸ¤—`Huggingface Tranformers`ğŸ¤— ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì´ìš©í•˜ì—¬ êµ¬í˜„

## Dependencies

- torch==1.4.0
- transformers==2.10.0
- sentencepiece==0.1.97

## Train

```bash
$ python3 main.py --model_type kobert --do_train --do_eval
```

## Prediction

 - [ì—¬ê¸°](https://drive.google.com/drive/folders/1-83lNwn58RE9bVOKKJ_h0uMzvrx587AU?usp=sharing)ì„œ pre-trained íŒŒì¼ë“¤ì„ ë‹¤ìš´ ë°›ì•„ ```./model``` ë””ë ‰í† ë¦¬ì— ë„£ì–´ì¤ë‹ˆë‹¤.


```bash
$ python3 predict.py --input_file {INPUT_FILE_PATH} --output_file {OUTPUT_FILE_PATH} --model_dir {SAVED_CKPT_PATH}
```
default ```INPUT_FILE_PATH``` : ```./sample_pred_in.txt```

default ```OUTPUT_FILE_PATH``` : ```./sample_pred_out.txt```

default ```SAVED_CKPT_PATH``` : ```./model```

## Results

|                   | Accuracy (%) |
| ----------------- | ------------ |
| KoBERT            | **89.63**    |
| DistilKoBERT      | 88.41        |
| Bert-Multilingual | 87.07        |
| FastText          | 85.50        |

## References

- [KoBERT](https://github.com/SKTBrain/KoBERT)
- [Huggingface Transformers](https://github.com/huggingface/transformers)
- [NSMC dataset](https://github.com/e9t/nsmc)

## Acknowledgments
* [ì›ì‘ì](https://github.com/monologg)ë¶„ê»˜ ì½”ë“œ ê³µìœ ì™€ ê´€ë ¨í•˜ì—¬ ê°ì‚¬ì˜ ë§ì”€ ì „í•©ë‹ˆë‹¤ :)
